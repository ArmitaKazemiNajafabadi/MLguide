# Machine Learning Guide

> A comprehensive guide from basics to advanced ML, RAG, and Agentic AI systems

---

## Table of Contents

### Part 1: Foundations
1. [Introduction to Machine Learning](#1-introduction-to-machine-learning)
2. [Mathematics for ML](#2-mathematics-for-ml)
3. [Data Preprocessing](#3-data-preprocessing)
4. [Classical ML Algorithms](#4-classical-ml-algorithms)

### Part 2: Deep Learning
5. [Neural Networks Fundamentals](#5-neural-networks-fundamentals)
6. [Convolutional Neural Networks (CNNs)](#6-convolutional-neural-networks-cnns)
7. [Recurrent Neural Networks (RNNs)](#7-recurrent-neural-networks-rnns)
8. [Transformers Architecture](#8-transformers-architecture)

### Part 3: Generative AI & Applications
9. [Generative AI Applications](#9-generative-ai-applications)
10. [Retrieval-Augmented Generation (RAG)](#10-retrieval-augmented-generation-rag)
11. [Multimodal AI](#11-multimodal-ai)
12. [Agentic AI Systems](#12-agentic-ai-systems)

---

## Part 1: Foundations

### 1. Introduction to Machine Learning
*[Content to be added]*

**Keywords:** supervised learning, unsupervised learning, reinforcement learning, training data, test data, model evaluation

---

### 2. Mathematics for ML
*[Content to be added]*

**Keywords:** linear algebra, calculus, probability, statistics, gradient descent, loss functions

---

### 3. Data Preprocessing
*[Content to be added]*

**Keywords:** normalization, standardization, missing values, feature engineering, encoding, train-test split

---

### 4. Classical ML Algorithms
*[Content to be added]*

**Keywords:** linear regression, logistic regression, decision trees, SVM, k-means, PCA

---

## Part 2: Deep Learning

### 5. Neural Networks Fundamentals
*[Content to be added]*

**Keywords:** perceptron, activation functions, backpropagation, optimization, regularization

---

### 6. Convolutional Neural Networks (CNNs)
*[Content to be added]*

**Keywords:** convolution, pooling, filters, image classification, computer vision

---

### 7. Recurrent Neural Networks (RNNs)
*[Content to be added]*

**Keywords:** LSTM, GRU, sequence modeling, time series, vanishing gradients

---

### 8. Transformers Architecture
*[Content to be added]*

**Keywords:** attention mechanism, self-attention, encoder-decoder, positional encoding, BERT, GPT

---

## Part 3: Generative AI & Applications

### 9. Generative AI Applications

#### 9.1 Discriminative vs Generative AI

**Discriminative AI:**
- **Purpose:** Distinguish between data classes
- **Limitation:** Cannot generate new data
- **Examples:** Classification, detection tasks
- **Method:** Learns decision boundaries

**Generative AI:**
- **Purpose:** Captures distribution of training data to generate new samples
- **Capability:** Creates novel content (text/image/video/audio/code)
- **Foundation:** Both based on deep learning architectures
- **Key difference:** Models probability distribution P(X) rather than P(Y|X)

**Keywords:** discriminative models, generative models, data distribution, synthesis, creativity

---

#### 9.2 Generative AI Models

**Core Architectures:**

1. **GANs (Generative Adversarial Networks)** - 2014
   - Generator vs Discriminator
   - Adversarial training

2. **VAEs (Variational Autoencoders)**
   - Latent space representation
   - Probabilistic encoding

3. **Transformers** - 2017
   - Attention mechanism
   - Sequence-to-sequence modeling

4. **Diffusion Models**
   - Iterative denoising
   - High-quality image generation

**Keywords:** GAN, VAE, transformer, diffusion, adversarial training, latent space, denoising

---

#### 9.3 Historical Timeline

| Year | Milestone |
|------|-----------|
| 1950s | **Machine Learning** foundations |
| 2014 | **GANs** introduced |
| 2017 | **Transformer** architecture ("Attention is All You Need") |
| 2018 | **GPT** (Generative Pre-trained Transformer) by OpenAI |
| 2020+ | GPT-3, DALL-E, Stable Diffusion era |

**Pre-2018 LLMs:**
- RNN-based (LSTM/GRU)
- Limited context window
- Sequential processing bottleneck
- Examples: Word2Vec, ELMo

**Post-2018 (Transformer era):**
- Parallel processing
- Attention mechanism
- Scalable to billions of parameters
- Transfer learning via pre-training

**Keywords:** ML history, deep learning evolution, LLM development, transformer revolution

---

#### 9.4 Major LLM Families

**Text Generation:**
- **GPT series** (OpenAI): GPT-3, GPT-4, ChatGPT
- **PaLM** (Google): Pathways Language Model
- **LLaMA** (Meta): Large Language Model Meta-AI
- **Gemini** (Google): Multimodal capabilities

**Key capabilities:** Coherent text generation, contextual understanding, few-shot learning, instruction following

**Keywords:** LLM, GPT, PaLM, LLaMA, Gemini, language models, pre-training, fine-tuning

---

#### 9.5 Generative AI Tools by Domain

**Text Generation:**
- ChatGPT (OpenAI)
- Gemini (Google)
- Claude (Anthropic)

**Image Generation:**
- DALL-E 2 (OpenAI)
- Stable Diffusion (Stability AI)
- Midjourney
- Adobe Firefly

**Video Generation:**
- Synthesia
- Runway
- Pika

**Code Generation:**
- GitHub Copilot
- AlphaCode (DeepMind)
- Replit Ghostwriter

**Audio Generation:**
- ElevenLabs
- Whisper (OpenAI)

**Keywords:** ChatGPT, DALL-E, Stable Diffusion, Midjourney, Synthesia, Copilot, AlphaCode, text-to-image, text-to-video, code completion

**External Resources:**
- [OpenAI Platform](https://platform.openai.com/)
- [Hugging Face Models](https://huggingface.co/models)
- [Stability AI](https://stability.ai/)

---

### 10. Retrieval-Augmented Generation (RAG)
*[Content to be added - Your course notes will go here]*

**Keywords:** retrieval, vector database, embeddings, context augmentation, grounding

---

### 11. Multimodal AI
*[Content to be added - Your course notes will go here]*

**Keywords:** vision-language models, CLIP, cross-modal learning, multimodal fusion

---

### 12. Agentic AI Systems
*[Content to be added - Your course notes will go here]*

**Keywords:** autonomous agents, tool use, planning, memory, reasoning

---

## Code Examples

- `example_linear_regression.py` - Basic regression implementation
- `example_neural_network.py` - Simple neural network from scratch
- `example_rag_system.py` - RAG implementation
- *(More examples to be added)*

---

## Resources & References

### Courses
- IBM RAG and Agentic AI Course (current source)

### Papers
- "Attention is All You Need" (Transformer paper)
- "Generative Adversarial Networks" (GAN paper)

### Documentation
- *(To be added)*

---

**Last Updated:** 2 December 2025  
**Status:** Work in Progress - Building from multiple sections
<img width="462" height="630" alt="image" src="https://github.com/user-attachments/assets/1899d5ad-da6b-41bf-be5c-0d3434518149" />
